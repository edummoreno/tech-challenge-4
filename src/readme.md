<!-- readme_src.md -->

# Documenta√ß√£o T√©cnica (Pipeline A ‚Üí B ‚Üí C)

Este documento descreve o funcionamento interno do pipeline, com **m√©tricas**, **f√≥rmulas** e **decis√µes de implementa√ß√£o** ‚Äî com foco especial nas **heur√≠sticas e otimiza√ß√µes do Step A**.

---

## 1) Fluxo geral do pipeline

### A) Step A ‚Äî Faces + Identidade + Emo√ß√µes (`step_a_faces_emotions.py`)
Entrada: `data/input.mp4`  
Sa√≠das:
- `outputs/stepA_annotated.mp4`
- `outputs/stepA_summary.txt`

Processo (alto n√≠vel):
1. Leitura de frames (OpenCV) com **I/O em thread** (FileVideoStream)
2. **Amostragem temporal** via `FRAME_STEP`
3. Detec√ß√£o de faces (DeepFace.extract_faces) em **frame reduzido** (`SCALE_DETECCAO`)
4. Reproje√ß√£o de bounding box para o frame original
5. **Heur√≠sticas de filtragem** (√°rea, aspect ratio, confian√ßa e persist√™ncia temporal)
6. Recorte do rosto com margem (`PAD_RATIO`)
7. Emo√ß√£o (DeepFace.analyze) com **skip + fallback**
8. Identidade (face_recognition) via dist√¢ncia e limiar
9. Anota√ß√£o no frame + resumo

---

### B) Step B ‚Äî Atividades / Pose (`step_b_activities.py`)
Entrada t√≠pica: v√≠deo do Step A (ou o original, conforme config do projeto)  
Sa√≠das:
- `outputs/stepB_annotated.mp4`
- `outputs/stepB_summary.txt`

Processo:
1. Leitura frame a frame
2. Pose (MediaPipe)
3. Heur√≠sticas para classificar atividades
4. Contagem em **frames por atividade**
5. Anota√ß√£o no frame + resumo

---

### C) Step C ‚Äî Consolida√ß√£o (`step_c_summary.py`)
Entrada:
- `outputs/stepA_summary.txt`
- `outputs/stepB_summary.txt`

Sa√≠da:
- `outputs/relatorio_final.txt`

Objetivo:
- Consolidar resultados
- Inserir um **üìä CONTEXTO DE PROCESSAMENTO** para evitar interpreta√ß√µes erradas (ex.: somar frames do Step A e B)

---

## 2) Configura√ß√µes relevantes (Step A)

As configs ficam em `criar_config()` (nomes t√≠picos):

- `VIDEO_ENTRADA`: `data/input.mp4`
- `PASTA_FACES_CONHECIDAS`: `data/known_faces`
- `VIDEO_SAIDA`: `outputs/stepA_annotated.mp4`
- `RESUMO_SAIDA`: `outputs/stepA_summary.txt`

### Performance
- `FRAME_STEP` (ex.: 3)
- `SCALE_DETECCAO` (ex.: 0.7)
- `align=False` na detec√ß√£o (mais r√°pido)
- Leitura em thread com fila (reduz gargalo de I/O)

### Robustez
- `DETECTOR_BACKEND = "opencv"`
- `ENFORCE_DETECTION = False`
- Warm-up + autoajuste de limiares (percentis)
- Filtros: √°rea, AR (aspect ratio), confian√ßa
- Persist√™ncia temporal (`K_PERSISTENCIA`, `TAMANHO_GRID`)
- `PAD_RATIO` para recorte de rosto com margem
- Emo√ß√£o com fallback

---

## 3) Heur√≠sticas e Otimiza√ß√µes do Step A (com f√≥rmulas)

Esta se√ß√£o detalha o ‚Äúporqu√™‚Äù do Step A ser **r√°pido e robusto**, e como cada heur√≠stica √© calculada.

---

### 3.1) Leitura otimizada de v√≠deo (I/O em thread)

**Problema:** leitura de frames pode virar gargalo (I/O), especialmente quando o processamento do frame √© pesado (DeepFace + face_recognition).

**Solu√ß√£o:** `FileVideoStream` l√™ frames em uma thread e guarda em uma fila (`queue`).  
- Benef√≠cio: computa√ß√£o e I/O ficam desacoplados, reduzindo ‚Äúparadas‚Äù na CPU/loop principal.
- Par√¢metros t√≠picos:
  - `queue_size = 128` (tamanho da fila)
  - thread daemon

**Modelo mental:**
- Thread A: l√™ `frame_t` e enfileira
- Thread B (main): consome `frame_t` e processa

---

### 3.2) Amostragem temporal (`FRAME_STEP`)

**Objetivo:** reduzir custo total processando apenas parte dos frames.

Se:
- `F_total` = total de frames do v√≠deo
- `step` = `FRAME_STEP`

Ent√£o uma aproxima√ß√£o √©:

- **F_analisados_A ‚âà ceil(F_total / step)**

Exemplo:
- `F_total = 3951`, `step = 3`
- `F_analisados_A ‚âà ceil(3951/3) = 1317`

**Trade-off:**
- ‚úÖ Reduz custo ~ proporcional a `step`
- ‚ö†Ô∏è Pode perder eventos muito r√°pidos entre frames amostrados

---

### 3.3) Downscale na detec√ß√£o (`SCALE_DETECCAO`) + reproje√ß√£o da bbox

**Objetivo:** detec√ß√£o de faces √© cara porque roda em cima de muitos pixels.

Se:
- `S` = `SCALE_DETECCAO` (ex.: 0.7)
- `W, H` = dimens√µes do frame original
- `W' = S¬∑W`, `H' = S¬∑H` = dimens√µes do frame reduzido

Ent√£o:
- **Pixels reduzidos = (W'¬∑H') = (S¬≤ ¬∑ W¬∑H)**

Ou seja, o custo de detec√ß√£o tende a cair aproximadamente com **S¬≤**.

Exemplo com `S=0.7`:
- `S¬≤ = 0.49`  
- Processa ~49% dos pixels ‚Üí ganho t√≠pico ~2x (aprox.) na etapa de detec√ß√£o.

#### Reproje√ß√£o de bbox (do frame pequeno para o original)

A detec√ß√£o retorna uma bbox no frame reduzido:
- `(x_s, y_s, w_s, h_s)`

Para desenhar e recortar corretamente no frame original:
- **x = floor(x_s / S)**
- **y = floor(y_s / S)**
- **w = floor(w_s / S)**
- **h = floor(h_s / S)**

Depois √© aplicado **clamp** para manter bbox dentro da imagem:
- `x ‚àà [0, W-1]`
- `y ‚àà [0, H-1]`
- `w ‚àà [1, W-x]`
- `h ‚àà [1, H-y]`

---

### 3.4) Warm-up + autoajuste estat√≠stico de limiares (percentis)

**Problema:** valores de √°rea da face, aspect ratio e confian√ßa variam muito de v√≠deo para v√≠deo.

**Solu√ß√£o:** no in√≠cio, coletar amostras (warm-up) e definir limiares por percentis.

#### Coleta de amostras (warm-up)
Durante o warm-up, para cada detec√ß√£o v√°lida:
- calcula `area = w¬∑h`
- calcula `AR = w/h`
- armazena `confidence` quando existir

**Filtro de outliers no warm-up:**
- ignora faces muito grandes:
  - **area < 0.6 ¬∑ area_frame**

onde:
- `area_frame = W¬∑H` do v√≠deo

#### Defini√ß√£o dos limiares por percentis
Depois de acumular N amostras (ex.: `FRAMES_WARMUP_ANALISADOS = 150`), define:

- **MIN_AREA_FACE = P10(area)**
- **MAX_AREA_FACE = P95(area)**
- **MIN_AR = P5(AR)**
- **MAX_AR = P95(AR)**
- **MIN_CONFIANCA = P20(confidence)** (se houver amostras), sen√£o 0.0

onde `Pk` √© o k-√©simo percentil.

**Fallback do AR (robustez):**  
Se o intervalo ficar ‚Äúapertado demais‚Äù:
- se `(MAX_AR - MIN_AR) < 0.15`, ent√£o:
  - `MIN_AR = 0.6`
  - `MAX_AR = 1.6`

**Motivo:** evita rejeitar tudo quando o percentil colou por distribui√ß√£o ruim/curta.

---

### 3.5) Filtros geom√©tricos (√°rea e aspect ratio)

Ap√≥s definir/usar limiares, uma detec√ß√£o s√≥ passa se:

- **MIN_AREA_FACE ‚â§ area ‚â§ MAX_AREA_FACE**
- **MIN_AR ‚â§ AR ‚â§ MAX_AR**

Onde:
- `area = w¬∑h`
- `AR = w/h`

**Efeito:** reduz falsos positivos:
- ‚Äúquadradinhos‚Äù muito pequenos
- bboxes muito esticadas (AR fora do padr√£o de face)

---

### 3.6) Filtro de confian√ßa do detector

Se o backend retornar `confidence`:
- a bbox s√≥ passa se:
  - **confidence ‚â• MIN_CONFIANCA**

Se **n√£o houver confidence** (ou vier None):
- a detec√ß√£o √© aceita (n√£o penaliza backends que n√£o fornecem score).

**Efeito:** reduz falsos positivos em cenas dif√≠ceis.

---

### 3.7) Persist√™ncia temporal por grid (reduzir ‚Äúpiscadas‚Äù)

**Problema:** detec√ß√µes inst√°veis podem ‚Äúpiscando‚Äù (aparecem em 1 frame e somem no seguinte).

**Solu√ß√£o:** s√≥ aceitar uma face se ela persistir por `K` ocorr√™ncias recentes dentro de uma mesma regi√£o (grid).

Define-se um ID aproximado para a face:
- **id_face = (round(x / grid), round(y / grid))**
onde:
- `grid = TAMANHO_GRID` (ex.: 60)

Mant√©m-se um hist√≥rico (deque):
- `historico_ids` com `maxlen` (ex.: 10)

Crit√©rio de aceita√ß√£o:
- se `K <= 1`: passa sempre
- sen√£o:
  - **count(historico_ids == id_face) ‚â• K**

**Interpreta√ß√£o:**
- A face precisa aparecer ‚Äúno mesmo quadrante‚Äù pelo menos `K` vezes no hist√≥rico recente.

**Efeito pr√°tico:**
- ‚úÖ reduz falsos positivos intermitentes
- ‚ö†Ô∏è pode atrasar a primeira apari√ß√£o em ~K amostras (trade-off)

---

### 3.8) Recorte do rosto com padding (PAD_RATIO)

Para melhorar emo√ß√£o/identidade, o recorte inclui margem ao redor da face.

Se:
- bbox = `(x, y, w, h)`
- `pad_ratio` = `PAD_RATIO`

Ent√£o:
- **pad = pad_ratio ¬∑ max(w, h)**

E o recorte vira:
- `x1 = max(0, x - pad)`
- `y1 = max(0, y - pad)`
- `x2 = min(W, x + w + pad)`
- `y2 = min(H, y + h + pad)`

**Efeito:**
- reduz cortes ‚Äúapertados‚Äù que atrapalham emo√ß√£o
- d√° mais contexto para landmarks (mesmo com align=False)

---

### 3.9) Emo√ß√£o (DeepFace) ‚Äî estrat√©gia ‚Äúskip + fallback‚Äù

**Objetivo:** reduzir custo evitando redetec√ß√£o dentro do `DeepFace.analyze`.

Estrat√©gia:
1) tenta:
   - `detector_backend="skip"`
   - `enforce_detection=False`
2) se falhar, fallback:
   - `DeepFace.analyze` padr√£o com `enforce_detection=False`

**Corre√ß√£o adicional (robustez):**
- se o crop for muito pequeno (ex.: <48√ó48), redimensiona para 96√ó96 antes da an√°lise.

**Efeito:**
- ‚úÖ acelera quando o crop j√° √© confi√°vel
- ‚úÖ mant√©m robustez quando o skip falha

---

### 3.10) Identidade (face_recognition) ‚Äî dist√¢ncia e limiar

Pipeline:
1. converte `crop` para RGB
2. calcula encoding no crop inteiro (localiza√ß√£o conhecida)
3. calcula dist√¢ncia para a base de encodings conhecidos:
   - `dist_i = face_distance(known_encodings, enc_atual)`
4. escolhe o menor:
   - `i* = argmin(dist_i)`

Regra:
- se **dist(i*) < 0.55**, ent√£o:
  - identidade = `known_names[i*]`
- sen√£o:
  - identidade = `"Desconhecido"`

**Efeito:**
- limiar menor ‚Üí menos falsos positivos (mais conservador)
- limiar maior ‚Üí mais matches (maior risco de confundir)

---

## 4) F√≥rmulas e M√©tricas (resumo)

### 4.1) Frames analisados no Step A (amostragem)
- **F_analisados_A ‚âà ceil(F_total / FRAME_STEP)**

### 4.2) Frames analisados no Step B (frame a frame)
- **F_analisados_B = F_total**

### 4.3) Redu√ß√£o de pixels na detec√ß√£o por downscale
- **pixels_small = S¬≤ ¬∑ pixels_original**
- ganho t√≠pico ~ proporcional a `1/S¬≤`

### 4.4) Contagem de atividades (Step B)
Para cada atividade `k`:
- **count(k) = Œ£ I(atividade_t = k)**, para `t = 1..F_total`

### 4.5) Contagem de emo√ß√µes (Step A)
No Step A, em frames amostrados e faces v√°lidas:
- **count(emocao e) = Œ£ Œ£ I(emocao(face_i, frame_t) = e)**

---

## 5) Contexto de Processamento (relat√≥rio final)

O Step C inclui:

```text
üìä CONTEXTO DE PROCESSAMENTO

- Total de frames do v√≠deo: <F_total>

Step A ‚Äî Emo√ß√µes e Faces:
- Frames analisados: <F_analisados_A>
- Estrat√©gia: amostragem temporal (1 a cada <FRAME_STEP> frames)

Step B ‚Äî Atividades Corporais:
- Frames analisados: <F_analisados_B>
- Estrat√©gia: an√°lise frame a frame
